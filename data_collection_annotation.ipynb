{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hedi11999/crop_classification_scotland_using_satelitte_imagery_major_tom/blob/main/data_collection_annotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3DK5qEw2H4p"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio\n",
        "!pip install h5py\n",
        "!pip install folium geopandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Major TOM Code Integration\n",
        "\n",
        "The following code has been sourced from the Major TOM project on GitHub. This code is designed for handling and processing geospatial data, including reading, filtering, and visualizing satellite imagery. We have combined several modules from the Major TOM repository into a single notebook to streamline the data collection and processing workflow. This integration allows us to easily work with the dataset in a unified environment.\n"
      ],
      "metadata": {
        "id": "2KL-FAKSXX27"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9hMHa5IHoZf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "import rasterio as rio\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class MajorTOM(Dataset):\n",
        "    \"\"\"MajorTOM Dataset (https://huggingface.co/Major-TOM)\n",
        "\n",
        "    Args:\n",
        "        df ((geo)pandas.DataFrame): Metadata dataframe\n",
        "        local_dir (string): Root directory of the local dataset version\n",
        "        tif_bands (list): A list of tif file names to be read\n",
        "        png_bands (list): A list of png file names to be read\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 df,\n",
        "                 local_dir = None,\n",
        "                 tif_bands=['B04','B03','B02'],\n",
        "                 png_bands=['thumbnail'],\n",
        "                 tif_transforms=[transforms.ToTensor()],\n",
        "                 png_transforms=[transforms.ToTensor()]\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.df = df\n",
        "        self.local_dir = Path(local_dir) if isinstance(local_dir,str) else local_dir\n",
        "        self.tif_bands = tif_bands if not isinstance(tif_bands,str) else [tif_bands]\n",
        "        self.png_bands = png_bands if not isinstance(png_bands,str) else [png_bands]\n",
        "        self.tif_transforms = transforms.Compose(tif_transforms) if tif_transforms is not None else None\n",
        "        self.png_transforms = transforms.Compose(png_transforms) if png_transforms is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def print_columns(self):\n",
        "        print(\"Columns:\")\n",
        "        for column in self.df.columns:\n",
        "            print(column)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        meta = self.df.iloc[idx]\n",
        "\n",
        "        product_id = meta.product_id\n",
        "        grid_cell = meta.grid_cell\n",
        "        row = grid_cell.split('_')[0]\n",
        "\n",
        "        path = self.local_dir / Path(\"{}/{}/{}\".format(row, grid_cell, product_id))\n",
        "        out_dict = {'meta' : meta}\n",
        "\n",
        "        for band in self.tif_bands:\n",
        "            with rio.open(path / '{}.tif'.format(band)) as f:\n",
        "                out = f.read()\n",
        "            if self.tif_transforms is not None:\n",
        "                out = self.tif_transforms(out)\n",
        "            out_dict[band] = out\n",
        "\n",
        "\n",
        "        for band in self.png_bands:\n",
        "            out = Image.open(path / '{}.png'.format(band))\n",
        "            if self.png_transforms is not None:\n",
        "                out = self.png_transforms(out)\n",
        "            out_dict[band] = out\n",
        "\n",
        "        return out_dict\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import LineString, Polygon\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class Grid():\n",
        "\n",
        "    RADIUS_EQUATOR = 6378.137 # km\n",
        "\n",
        "    def __init__(self,dist,latitude_range=(-85,85),longitude_range=(-180,180),utm_definition='bottomleft'):\n",
        "        self.dist = dist\n",
        "        self.latitude_range = latitude_range\n",
        "        self.longitude_range = longitude_range\n",
        "        self.utm_definition = utm_definition\n",
        "        self.rows,self.lats = self.get_rows()\n",
        "        self.points, self.points_by_row = self.get_points()\n",
        "\n",
        "    def get_rows(self):\n",
        "\n",
        "        # Define set of latitudes to use, based on the grid distance\n",
        "        arc_pole_to_pole = math.pi * self.RADIUS_EQUATOR\n",
        "        num_divisions_in_hemisphere = math.ceil(arc_pole_to_pole / self.dist)\n",
        "\n",
        "        latitudes = np.linspace(-90, 90, num_divisions_in_hemisphere+1)[:-1]\n",
        "        latitudes = np.mod(latitudes, 180) - 90\n",
        "\n",
        "        # order should be from south to north\n",
        "        latitudes = np.sort(latitudes)\n",
        "\n",
        "        zeroth_row = np.searchsorted(latitudes,0)\n",
        "\n",
        "        # From 0U-NU and 1D-ND\n",
        "        rows = [None] * len(latitudes)\n",
        "        rows[zeroth_row:] = [f'{i}U' for i in range(len(latitudes)-zeroth_row)]\n",
        "        rows[:zeroth_row] = [f'{abs(i-zeroth_row)}D' for i in range(zeroth_row)]\n",
        "\n",
        "        # bound to range\n",
        "        idxs = (latitudes>=self.latitude_range[0]) * (latitudes<=self.latitude_range[1])\n",
        "        rows,latitudes = np.array(rows), np.array(latitudes)\n",
        "        rows,latitudes = rows[idxs],latitudes[idxs]\n",
        "\n",
        "        return rows,latitudes\n",
        "\n",
        "    def get_circumference_at_latitude(self,lat):\n",
        "\n",
        "        # Circumference of the cross-section of a sphere at a given latitude\n",
        "\n",
        "        radius_at_lat = self.RADIUS_EQUATOR * math.cos(lat * math.pi / 180)\n",
        "        circumference = 2 * math.pi * radius_at_lat\n",
        "\n",
        "        return circumference\n",
        "\n",
        "    def subdivide_circumference(self,lat,return_cols=False):\n",
        "        # Provide a list of longitudes that subdivide the circumference of the earth at a given latitude\n",
        "        # into equal parts as close as possible to dist\n",
        "\n",
        "        circumference = self.get_circumference_at_latitude(lat)\n",
        "        num_divisions = math.ceil(circumference / self.dist)\n",
        "        longitudes = np.linspace(-180,180, num_divisions+1)[:-1]\n",
        "        longitudes = np.mod(longitudes, 360) - 180\n",
        "        longitudes = np.sort(longitudes)\n",
        "\n",
        "\n",
        "        if return_cols:\n",
        "            cols = [None] * len(longitudes)\n",
        "            zeroth_idx = np.where(longitudes==0)[0][0]\n",
        "            cols[zeroth_idx:] = [f'{i}R' for i in range(len(longitudes)-zeroth_idx)]\n",
        "            cols[:zeroth_idx] = [f'{abs(i-zeroth_idx)}L' for i in range(zeroth_idx)]\n",
        "            return np.array(cols),np.array(longitudes)\n",
        "\n",
        "        return np.array(longitudes)\n",
        "\n",
        "    def get_points(self):\n",
        "\n",
        "        r_idx = 0\n",
        "        points_by_row = [None]*len(self.rows)\n",
        "        for r,lat in zip(self.rows,self.lats):\n",
        "            point_names,grid_row_names,grid_col_names,grid_row_idx,grid_col_idx,grid_lats,grid_lons,utm_zones,epsgs = [],[],[],[],[],[],[],[],[]\n",
        "            cols,lons = self.subdivide_circumference(lat,return_cols=True)\n",
        "\n",
        "            cols,lons = self.filter_longitude(cols,lons)\n",
        "            c_idx = 0\n",
        "            for c,lon in zip(cols,lons):\n",
        "                point_names.append(f'{r}_{c}')\n",
        "                grid_row_names.append(r)\n",
        "                grid_col_names.append(c)\n",
        "                grid_row_idx.append(r_idx)\n",
        "                grid_col_idx.append(c_idx)\n",
        "                grid_lats.append(lat)\n",
        "                grid_lons.append(lon)\n",
        "                if self.utm_definition == 'bottomleft':\n",
        "                    utm_zones.append(get_utm_zone_from_latlng([lat,lon]))\n",
        "                elif self.utm_definition == 'center':\n",
        "                    center_lat = lat + (1000*self.dist/2)/111_120\n",
        "                    center_lon = lon + (1000*self.dist/2)/(111_120*math.cos(center_lat*math.pi/180))\n",
        "                    utm_zones.append(get_utm_zone_from_latlng([center_lat,center_lon]))\n",
        "                else:\n",
        "                    raise ValueError(f'Invalid utm_definition {self.utm_definition}')\n",
        "                epsgs.append(f'EPSG:{utm_zones[-1]}')\n",
        "\n",
        "                c_idx += 1\n",
        "            points_by_row[r_idx] = gpd.GeoDataFrame({\n",
        "                'name':point_names,\n",
        "                'row':grid_row_names,\n",
        "                'col':grid_col_names,\n",
        "                'row_idx':grid_row_idx,\n",
        "                'col_idx':grid_col_idx,\n",
        "                'utm_zone':utm_zones,\n",
        "                'epsg':epsgs\n",
        "            },geometry=gpd.points_from_xy(grid_lons,grid_lats))\n",
        "            r_idx += 1\n",
        "        points = gpd.GeoDataFrame(pd.concat(points_by_row))\n",
        "        # points.reset_index(inplace=True,drop=True)\n",
        "        return points, points_by_row\n",
        "\n",
        "    def group_points_by_row(self):\n",
        "        # Make list of different gdfs for each row\n",
        "        points_by_row = [None]*len(self.rows)\n",
        "        for i,row in enumerate(self.rows):\n",
        "            points_by_row[i] = self.points[self.points.row==row]\n",
        "        return points_by_row\n",
        "\n",
        "    def filter_longitude(self,cols,lons):\n",
        "        idxs = (lons>=self.longitude_range[0]) * (lons<=self.longitude_range[1])\n",
        "        cols,lons = cols[idxs],lons[idxs]\n",
        "        return cols,lons\n",
        "\n",
        "    def latlon2rowcol(self,lats,lons,return_idx=False,integer=False):\n",
        "        \"\"\"\n",
        "        Convert latitude and longitude to row and column number from the grid\n",
        "        \"\"\"\n",
        "        # Always take bottom left corner of grid cell\n",
        "        rows = np.searchsorted(self.lats,lats)-1\n",
        "\n",
        "        # Get the possible points of the grid cells at the given latitude\n",
        "        possible_points = [self.points_by_row[row] for row in rows]\n",
        "\n",
        "        # For each point, find the rightmost point that is still to the left of the given longitude\n",
        "        cols = [poss_points.iloc[np.searchsorted(poss_points.geometry.x,lon)-1].col for poss_points,lon in zip(possible_points,lons)]\n",
        "        rows = self.rows[rows].tolist()\n",
        "\n",
        "        outputs = [rows, cols]\n",
        "        if return_idx:\n",
        "            # Get the table index for self.points with each row,col pair in rows, cols\n",
        "            idx = [self.points[(self.points.row==row) & (self.points.col==col)].index.values[0] for row,col in zip(rows,cols)]\n",
        "            outputs.append(idx)\n",
        "\n",
        "        # return raw numbers\n",
        "        if integer:\n",
        "            outputs[0] = [int(el[:-1]) if el[-1] == 'U' else -int(el[:-1]) for el in outputs[0]]\n",
        "            outputs[1] = [int(el[:-1]) if el[-1] == 'R' else -int(el[:-1]) for el in outputs[1]]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def rowcol2latlon(self,rows,cols):\n",
        "        point_geoms = [self.points.loc[(self.points.row==row) & (self.points.col==col),'geometry'].values[0] for row,col in zip(rows,cols)]\n",
        "        lats = [point.y for point in point_geoms]\n",
        "        lons = [point.x for point in point_geoms]\n",
        "        return lats,lons\n",
        "\n",
        "    def get_bounded_footprint(self,point,buffer_ratio=0):\n",
        "        # Gets the polygon footprint of the grid cell for a given point, bounded by the other grid points' cells.\n",
        "        # Grid point defined as bottom-left corner of polygon. Buffer ratio is the ratio of the grid cell's width/height to buffer by.\n",
        "\n",
        "        bottom,left = point.geometry.y,point.geometry.x\n",
        "        row = point.row\n",
        "        row_idx = point.row_idx\n",
        "        col_idx = point.col_idx\n",
        "        next_row_idx = row_idx+1\n",
        "        next_col_idx = col_idx+1\n",
        "\n",
        "        if next_row_idx >= len(self.lats): # If at top row, use difference between top and second-to-top row for height\n",
        "            height = (self.lats[row_idx] - self.lats[row_idx-1])\n",
        "            top = self.lats[row_idx] + height\n",
        "        else:\n",
        "            top = self.lats[next_row_idx]\n",
        "\n",
        "        max_col = len(self.points_by_row[row].col_idx)-1\n",
        "        if next_col_idx > max_col: # If at rightmost column, use difference between rightmost and second-to-rightmost column for width\n",
        "            width = (self.points_by_row[row].iloc[col_idx].geometry.x - self.points_by_row[row].iloc[col_idx-1].geometry.x)\n",
        "            right = self.points_by_row[row].iloc[col_idx].geometry.x + width\n",
        "        else:\n",
        "            right = self.points_by_row[row].iloc[next_col_idx].geometry.x\n",
        "\n",
        "        # Buffer the polygon by the ratio of the grid cell's width/height\n",
        "        width = right - left\n",
        "        height = top - bottom\n",
        "\n",
        "        buffer_horizontal = width * buffer_ratio\n",
        "        buffer_vertical = height * buffer_ratio\n",
        "\n",
        "        new_left = left - buffer_horizontal\n",
        "        new_right = right + buffer_horizontal\n",
        "\n",
        "        new_bottom = bottom - buffer_vertical\n",
        "        new_top = top + buffer_vertical\n",
        "\n",
        "        bbox = Polygon([(new_left,new_bottom),(new_left,new_top),(new_right,new_top),(new_right,new_bottom)])\n",
        "\n",
        "        return bbox\n",
        "\n",
        "\n",
        "def get_utm_zone_from_latlng(latlng):\n",
        "    \"\"\"\n",
        "    Get the UTM ZONE from a latlng list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    latlng : List[Union[int, float]]\n",
        "        The latlng list to get the UTM ZONE from.\n",
        "\n",
        "    return_epsg : bool, optional\n",
        "        Whether or not to return the EPSG code instead of the WKT, by default False\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The WKT or EPSG code.\n",
        "    \"\"\"\n",
        "    assert isinstance(latlng, (list, np.ndarray)), \"latlng must be in the form of a list.\"\n",
        "\n",
        "    zone = math.floor(((latlng[1] + 180) / 6) + 1)\n",
        "    n_or_s = \"S\" if latlng[0] < 0 else \"N\"\n",
        "\n",
        "    false_northing = \"10000000\" if n_or_s == \"S\" else \"0\"\n",
        "    central_meridian = str(zone * 6 - 183)\n",
        "    epsg = f\"32{'7' if n_or_s == 'S' else '6'}{str(zone)}\"\n",
        "\n",
        "    return epsg\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    dist = 100\n",
        "    grid = Grid(dist,latitude_range=(10,70),longitude_range=(-30,60))\n",
        "\n",
        "    from pprint import pprint\n",
        "\n",
        "    test_lons = np.random.uniform(-20,50,size=(1000))\n",
        "    test_lats = np.random.uniform(12,68,size=(1000))\n",
        "\n",
        "    test_rows,test_cols = grid.latlon2rowcol(test_lats,test_lons)\n",
        "    test_lats2,test_lons2 = grid.rowcol2latlon(test_rows,test_cols)\n",
        "\n",
        "    print(test_lons[:10])\n",
        "    print(test_lats[:10])\n",
        "    print(test_rows[:10])\n",
        "    print(test_cols[:10])\n",
        "\n",
        "    # Make line segments from the points to their corresponding grid points\n",
        "    lines = []\n",
        "    for i in range(len(test_lats)):\n",
        "        lines.append([(test_lons[i],test_lats[i]),(test_lons2[i],test_lats2[i])])\n",
        "\n",
        "    lines = gpd.GeoDataFrame(geometry=gpd.GeoSeries([LineString(line) for line in lines]))\n",
        "\n",
        "    lines.to_file(f'testlines_{dist}km.geojson',driver='GeoJSON')\n",
        "    grid.points.to_file(f'testgrid_{dist}km.geojson',driver='GeoJSON')\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import fsspec\n",
        "from fsspec.parquet import open_parquet_file\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from rasterio.io import MemoryFile\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "def metadata_from_url(access_url, local_url):\n",
        "    local_url, response = urllib.request.urlretrieve(access_url, local_url)\n",
        "    df = pq.read_table(local_url).to_pandas()\n",
        "    df['timestamp'] = pd.to_datetime(df.timestamp)\n",
        "    gdf = gpd.GeoDataFrame(\n",
        "        df, geometry=gpd.points_from_xy(df.centre_lon, df.centre_lat), crs=df.crs.iloc[0]\n",
        "    )\n",
        "    return gdf\n",
        "\n",
        "def filter_metadata(df,\n",
        "                    region=None,\n",
        "                    daterange=None,\n",
        "                    cloud_cover=(0,100),\n",
        "                    nodata=(0, 1.0)\n",
        "                   ):\n",
        "    \"\"\"Filters the Major-TOM dataframe based on several parameters\n",
        "\n",
        "    Args:\n",
        "        df (geopandas dataframe): Parent dataframe\n",
        "        region (shapely geometry object) : Region of interest\n",
        "        daterange (tuple) : Inclusive range of dates (example format: '2020-01-01')\n",
        "        cloud_cover (tuple) : Inclusive percentage range (0-100) of cloud cover\n",
        "        nodata (tuple) : Inclusive fraction (0.0-1.0) of no data allowed in a sample\n",
        "\n",
        "    Returns:\n",
        "        df: a filtered dataframe\n",
        "    \"\"\"\n",
        "    # temporal filtering\n",
        "    if daterange is not None:\n",
        "        assert (isinstance(daterange, list) or isinstance(daterange, tuple)) and len(daterange)==2\n",
        "        df = df[df.timestamp >= daterange[0]]\n",
        "        df = df[df.timestamp <= daterange[1]]\n",
        "\n",
        "    # spatial filtering\n",
        "    if region is not None:\n",
        "        idxs = df.sindex.query(region)\n",
        "        df = df.take(idxs)\n",
        "    # cloud filtering\n",
        "    if cloud_cover is not None:\n",
        "        df = df[df.cloud_cover >= cloud_cover[0]]\n",
        "        df = df[df.cloud_cover <= cloud_cover[1]]\n",
        "\n",
        "    # spatial filtering\n",
        "    if nodata is not None:\n",
        "        df = df[df.nodata >= nodata[0]]\n",
        "        df = df[df.nodata <= nodata[1]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def read_row(row, columns=[\"thumbnail\"]):\n",
        "    \"\"\"Reads a row from a Major-TOM dataframe\n",
        "\n",
        "    Args:\n",
        "        row (row from geopandas dataframe): The row of metadata\n",
        "        columns (list): columns to be read from the file\n",
        "\n",
        "    Returns:\n",
        "        data (dict): dictionary with returned data from requested columns\n",
        "    \"\"\"\n",
        "    with open_parquet_file(row.parquet_url,columns = columns) as f:\n",
        "        with pq.ParquetFile(f) as pf:\n",
        "            row_group = pf.read_row_group(row.parquet_row, columns=columns)\n",
        "\n",
        "    if columns == [\"thumbnail\"]:\n",
        "        stream = BytesIO(row_group['thumbnail'][0].as_py())\n",
        "        return Image.open(stream)\n",
        "    else:\n",
        "        row_output = {}\n",
        "        for col in columns:\n",
        "            bytes = row_group[col][0].as_py()\n",
        "\n",
        "            if col != 'thumbnail':\n",
        "                row_output[col] = read_tif_bytes(bytes)\n",
        "            else:\n",
        "                stream = BytesIO(bytes)\n",
        "                row_output[col] = Image.open(stream)\n",
        "\n",
        "        return row_output\n",
        "\n",
        "def filter_download(df, local_dir, source_name, by_row = False, verbose = False, tif_columns=None):\n",
        "    \"\"\"Downloads and unpacks the data of Major-TOM based on a metadata dataframe\n",
        "\n",
        "    Args:\n",
        "        df (geopandas dataframe): Metadata dataframe\n",
        "        local_dir (str or Path) : Path to the where the data is to be stored locally\n",
        "        source_name (str) : Name alias of the resulting dataset\n",
        "        by_row (bool): If True, it will access individual rows of parquet via http - otherwise entire parquets are downloaded temporarily\n",
        "        verbose (bool) : option for potential internal state printing\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(local_dir, str):\n",
        "        local_dir = Path(local_dir)\n",
        "\n",
        "    temp_file = local_dir / 'temp.parquet'\n",
        "\n",
        "    # identify all parquets that need to be downloaded (group them)\n",
        "    urls = df.parquet_url.unique()\n",
        "    print('Starting download of {} parquet files.'.format(len(urls))) if verbose else None\n",
        "\n",
        "    for url in tqdm(urls, desc='Downloading and unpacking...'):\n",
        "        # identify all relevant rows\n",
        "        rows = df[df.parquet_url == url].parquet_row.unique()\n",
        "\n",
        "        if not by_row: # (downloads entire parquet)\n",
        "            # download a temporary file\n",
        "            temp_path, http_resp = urllib.request.urlretrieve(url, temp_file)\n",
        "        else:\n",
        "            f=fsspec.open(url)\n",
        "            temp_path = f.open()\n",
        "\n",
        "        # populate the bands\n",
        "        with pq.ParquetFile(temp_path) as pf:\n",
        "            for row_idx in rows:\n",
        "                table = pf.read_row_group(row_idx)\n",
        "\n",
        "                product_id = table['product_id'][0].as_py()\n",
        "                grid_cell = table['grid_cell'][0].as_py()\n",
        "                row = grid_cell.split('_')[0]\n",
        "\n",
        "                dest = local_dir / Path(\"{}/{}/{}/{}\".format(source_name, row, grid_cell, product_id))\n",
        "                dest.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "                columns = [col for col in table.column_names if col[0] == 'B'] + ['cloud_mask'] if tif_columns is None else tif_columns\n",
        "                # tifs\n",
        "                for col in columns:\n",
        "                    with open(dest / \"{}.tif\".format(col), \"wb\") as f:\n",
        "                        # Write bytes to file\n",
        "                        f.write(table[col][0].as_py())\n",
        "\n",
        "                # thumbnail (png)\n",
        "                col = 'thumbnail'\n",
        "                with open(dest / \"{}.png\".format(col), \"wb\") as f:\n",
        "                    # Write bytes to file\n",
        "                    f.write(table[col][0].as_py())\n",
        "        if not by_row:\n",
        "            # remove downloaded file\n",
        "            os.remove(temp_path)\n",
        "        else:\n",
        "            f.close()\n",
        "\n",
        "from rasterio.io import MemoryFile\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def plot(sample, bands = ['B04', 'B03', 'B02'], scaling=2e3):\n",
        "    img = []\n",
        "    for b in bands:\n",
        "        img.append(read_tif_bytes(sample[b]))\n",
        "    plt.imshow(np.stack(img, -1)/2e3)\n",
        "\n",
        "def read_tif_bytes(tif_bytes):\n",
        "    with MemoryFile(tif_bytes) as mem_f:\n",
        "        with mem_f.open(driver='GTiff') as f:\n",
        "            return f.read().squeeze()\n",
        "\n",
        "def read_png_bytes(png_bytes):\n",
        "    stream = BytesIO(png_bytes)\n",
        "    return Image.open(stream)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Filtering and Download with Major TOM\n",
        "\n",
        "In this section, we utilize the Major TOM code from GitHub to filter and download Sentinel-2 satellite imagery data based on specific temporal and spatial parameters. The filtering process is configured to:\n",
        "\n",
        "1. **Temporal Range**: We filter the data for the year 2018, from January 1st, 2018, to January 1st, 2019.\n",
        "2. **Cloud Coverage**: We only include images with cloud coverage between 0% and 20%.\n",
        "3. **Region**: The geographical focus is on East Scotland, defined by a bounding box around agricultural regions near Dundee and Edinburgh.\n",
        "\n",
        "While this example focuses on Sentinel-2 data, it may be beneficial to also consider data from other satellites, such as combining it with Sentinel-1 SAR data. Exploring the use of both optical and radar imagery could provide a more comprehensive analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "nm-iLmHDXgQj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5c5ZVqdH8UH"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "SOURCE_DATASET = 'Major-TOM/Core-S2L2A' # Identify HF Dataset\n",
        "DATASET_DIR = Path('./data/Major-TOM/')\n",
        "DATASET_DIR.mkdir(exist_ok=True, parents=True)\n",
        "ACCESS_URL = 'https://huggingface.co/datasets/{}/resolve/main/metadata.parquet?download=true'.format(SOURCE_DATASET)\n",
        "LOCAL_URL = DATASET_DIR / '{}.parquet'.format(ACCESS_URL.split('.parquet')[0].split('/')[-1])\n",
        "\n",
        "# download from server to local url\n",
        "gdf = metadata_from_url(ACCESS_URL, LOCAL_URL)\n",
        "\n",
        "print(gdf.head())\n",
        "\n",
        "from shapely.geometry import box\n",
        "\n",
        "# Example bounding boxes used for filtering\n",
        "switzerland = box(5.9559111595,45.8179931641,10.4920501709,47.808380127)\n",
        "gabon = box(8.1283659854,-4.9213919841,15.1618722208,2.7923006325)\n",
        "napoli = box(14.091710578,40.7915558593,14.3723765416,40.9819258062)\n",
        "pacific = box(-153.3922893485,39.6170415622,-152.0423077748,40.7090892316) # a remote patch over pacific - no data\n",
        "scotland = box(-4.5, 55.0, -2.0, 57.0) # east Scotland agricultural regions arround dundee and arround edinburgh\n",
        "\n",
        "\n",
        "\n",
        "filtered_df = filter_metadata(gdf,\n",
        "                              cloud_cover = (0,20), # cloud cover between 0% and 20%\n",
        "                              region=scotland, # you can try with different bounding boxes, like in the cell above\n",
        "                              daterange=('2018-01-01', '2019-01-01'), # temporal range\n",
        "                              nodata=(0.0,0.0) # only 0% of no data allowed\n",
        "                              )\n",
        "\n",
        "print(filtered_df.head())\n",
        "\n",
        "filter_download(filtered_df, local_dir='./data/', source_name='L2A', by_row=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Annotation and Exploration\n",
        "\n",
        "With the Sentinel-2 data successfully downloaded, the next step involves exploring the dataset to understand its content and prepare for annotation. This section will begin by visualizing some of the images to get a sense of the data quality and coverage. We will also check the number of images available to us, which is crucial for planning the annotation process.\n"
      ],
      "metadata": {
        "id": "a3LEjvdHY3iv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-gmpfNzkIIm"
      },
      "outputs": [],
      "source": [
        "ds = MajorTOM(filtered_df, './data/L2A')\n",
        "\n",
        "ds[0]\n",
        "\n",
        "print(len(ds))\n",
        "\n",
        "print(type(ds))\n",
        "\n",
        "# Print the type and some example data to understand the structure of 'ds'\n",
        "print(\"Type of ds:\", type(ds))\n",
        "print(\"Type of first element in ds:\", type(ds[0]))\n",
        "print(\"First element in ds:\", ds[0])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining Crop Maps into a Single GeoDataFrame\n",
        "\n",
        "In this step, we will combine all crop map files from different regions into a single GeoDataFrame. This unified GeoDataFrame will contain all the polygons representing the crop maps for the specific region under study. By consolidating the data into one GeoDataFrame, we can streamline further analysis and processing tasks, ensuring that all relevant spatial information is easily accessible and manageable.\n"
      ],
      "metadata": {
        "id": "wEO0-2_tZPwo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc5o3TPhjOAU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import geopandas as gpd\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your GPKG folder in Google Drive\n",
        "gpkg_folder_path = '/content/drive/My Drive/MscProject/CropMaps/2023/'\n",
        "\n",
        "# List all GPKG files in the folder\n",
        "gpkg_files = glob.glob(os.path.join(gpkg_folder_path, '*.gpkg'))\n",
        "\n",
        "# Initialize an empty list to hold GeoDataFrames\n",
        "gdfs = []\n",
        "\n",
        "# Read each GPKG file and append the GeoDataFrame to the list\n",
        "for gpkg_file in gpkg_files:\n",
        "    gdf = gpd.read_file(gpkg_file)\n",
        "    gdfs.append(gdf)\n",
        "\n",
        "# Concatenate all GeoDataFrames into one\n",
        "combined_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
        "\n",
        "# Print the combined GeoDataFrame\n",
        "print(combined_gdf.head())\n",
        "\n",
        "# Check the CRS of the combined GeoDataFrame\n",
        "print(combined_gdf.crs)\n",
        "\n",
        "# If needed, transform the combined GeoDataFrame to the target CRS (for example, EPSG:4326 - WGS 84)\n",
        "target_crs = \"epsg:4326\"\n",
        "combined_gdf_transformed = combined_gdf.to_crs(target_crs)\n",
        "\n",
        "# Display the transformed GeoDataFrame\n",
        "print(combined_gdf_transformed.head())\n",
        "\n",
        "# Print unique values in the 'crop_code' column\n",
        "unique_crop_codes = combined_gdf_transformed['crop_code'].unique()\n",
        "print(\"Unique Crop Codes:\", unique_crop_codes)\n",
        "\n",
        "# Print unique values in the 'crop_name' column\n",
        "unique_crop_names = combined_gdf_transformed['crop_name'].unique()\n",
        "print(\"Unique Crop Names:\", unique_crop_names)\n",
        "\n",
        "# Example: Access the first polygon's coordinates\n",
        "if not combined_gdf_transformed.empty:\n",
        "    first_polygon = combined_gdf_transformed.loc[0, 'geometry']\n",
        "    if first_polygon.geom_type == 'Polygon':\n",
        "        first_polygon_coords = first_polygon.exterior.coords\n",
        "        # Display the coordinates\n",
        "        for coord in first_polygon_coords:\n",
        "            print(coord)\n",
        "    else:\n",
        "        print(\"The first geometry is not a Polygon.\")\n",
        "else:\n",
        "    print(\"The combined GeoDataFrame is empty.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Coordinates for Each Pixel in Each Image\n",
        "\n",
        "# Calculating Coordinates and Assigning Classes to Each Pixel\n",
        "\n",
        "In this step, we will calculate the geographic coordinates for every pixel in each image within our dataset. This is crucial for spatial analysis as it allows us to accurately map the pixel data to real-world locations.\n",
        "\n",
        "For each image, we will extract the georeferencing information (such as the affine transformation matrix) to determine the latitude and longitude of each pixel. These coordinates will then be stored in matrices, which can be used for further analysis and visualization.\n",
        "\n",
        "After calculating the coordinates, we will perform a left join between the pixel matrices and the crop map polygons. This will allow us to assign each pixel with the corresponding class (e.g., crop type) it belongs to. This step is essential for classifying the pixels based on their geographic location within the mapped regions.\n",
        "\n",
        "By mapping each pixel to its corresponding geographic coordinates and assigning the appropriate class, we can ensure that our analysis is both spatially accurate and contextually relevant, enabling precise alignment with other geospatial data layers and facilitating more detailed analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "pvPI0UBRaE0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XrhEl4uOUAoR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from shapely.geometry import Point\n",
        "import gc\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def calculate_pixel_coordinates(image_shape, central_coords, resolution_m):\n",
        "    height, width = image_shape\n",
        "    central_lat, central_lon = central_coords\n",
        "\n",
        "    lat_res = resolution_m / 111320\n",
        "    lon_res = resolution_m / (111320 * np.cos(np.radians(central_lat)))\n",
        "\n",
        "    y_indices, x_indices = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
        "\n",
        "    latitudes = central_lat + (y_indices - height // 2) * lat_res\n",
        "    longitudes = central_lon + (x_indices - width // 2) * lon_res\n",
        "\n",
        "    coords_matrix = np.stack([latitudes, longitudes], axis=-1)\n",
        "\n",
        "    return coords_matrix\n",
        "\n",
        "def create_point_matrix(coords_matrix):\n",
        "    height, width = coords_matrix.shape[:2]\n",
        "    point_matrix = np.empty((height, width), dtype=object)\n",
        "    for i in range(height):\n",
        "        for j in range(width):\n",
        "            lat, lon = coords_matrix[i, j]\n",
        "            point_matrix[i, j] = Point(lon, lat)\n",
        "    return point_matrix\n",
        "\n",
        "def process_single_image(image_shape, central_coords, resolution_m, idx, join_gdf):\n",
        "    coords_matrix = calculate_pixel_coordinates(image_shape, central_coords, resolution_m)\n",
        "    point_matrix = create_point_matrix(coords_matrix)\n",
        "    height, width = point_matrix.shape[:2]\n",
        "\n",
        "    points_list = [(i, j, point_matrix[i, j]) for i in range(height) for j in range(width)]\n",
        "    points_df = pd.DataFrame(points_list, columns=['i', 'j', 'geometry'])\n",
        "    gdf = gpd.GeoDataFrame(points_df, geometry='geometry')\n",
        "\n",
        "\n",
        "    joined_gdf = gpd.sjoin(gdf, join_gdf, how='left', op='within')\n",
        "    joined_gdf[\"crop_code\"] = joined_gdf[\"crop_code\"].fillna(\"un\")\n",
        "\n",
        "    result_matrix = np.empty((height, width), dtype=object)\n",
        "    for row in joined_gdf.itertuples():\n",
        "        result_matrix[row.i, row.j] = getattr(row, \"crop_code\", \"un\")\n",
        "\n",
        "    return result_matrix\n",
        "\n",
        "resolution_m = 10\n",
        "batch_size = 20\n",
        "\n",
        "drive_path = '/content/drive/My Drive/IntermediateResults'\n",
        "if not os.path.exists(drive_path):\n",
        "    os.makedirs(drive_path)\n",
        "\n",
        "hdf5_file_path = os.path.join(drive_path, 'all_data.h5')\n",
        "\n",
        "with h5py.File(hdf5_file_path, 'w') as h5f:\n",
        "    for idx, data in enumerate(ds):\n",
        "        print(f\"Processing image {idx + 1}/{len(ds)}\")\n",
        "\n",
        "        image_shape = data['thumbnail'].shape[1:3]\n",
        "        central_coords = (data['meta']['centre_lat'], data['meta']['centre_lon'])\n",
        "\n",
        "        result_matrix = process_single_image(image_shape, central_coords, resolution_m, idx, combined_gdf_transformed)\n",
        "\n",
        "        group = h5f.create_group(f'image_{idx}')\n",
        "        group.create_dataset('thumbnail', data=data['thumbnail'], compression='gzip')\n",
        "        group.create_dataset('result_matrix', data=result_matrix, compression='gzip')\n",
        "\n",
        "        meta_group = group.create_group('meta')\n",
        "        for key, value in data['meta'].items():\n",
        "            meta_group.attrs[key] = str(value)  # Convert all metadata values to strings\n",
        "\n",
        "        print(f\"Added result matrix to image {idx + 1}/{len(ds)}\")\n",
        "\n",
        "    print(f\"Saved all data in HDF5 format at {hdf5_file_path}.\")\n",
        "\n",
        "# Clean up to free memory\n",
        "vars_to_delete = ['result_matrix', 'coords_matrix', 'point_matrix', 'points_list', 'joined_gdf']\n",
        "for var in vars_to_delete:\n",
        "    if var in locals():\n",
        "        del locals()[var]\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "\n",
        "print(\"All result matrices added to the dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Colored Masks and Organizing Data for Further Analysis\n",
        "\n",
        "In this final step, we will generate colored masks for crop classification based on a predefined crop colors mapping. Each pixel in the image will be assigned a specific color corresponding to its crop class, resulting in a visual representation of crop types across the region.\n",
        "\n",
        "### Color Mapping and Mask Creation\n",
        "While we will use an initial color mapping for the crop classes, it's important to note that a more appropriate and thoughtful mapping should be developed. This will ensure that the colors used in the masks are intuitive and distinct, making the classification easier to interpret.\n",
        "\n",
        "### Restricting to Major Crops\n",
        "After creating the masks, we may also consider restricting the classification to only the major crops within the region. This step would simplify the classification task by focusing on the most significant crops, which could be more relevant for certain analyses or machine learning training.\n",
        "\n",
        "### Saving Image and Mask Pairs\n",
        "Finally, we will save each pair of the original image and its corresponding mask in a structured format. The data will be organized into folders by year and batch, ensuring that the dataset is well-organized and ready for further analysis or model training. This organization will facilitate easy access and management of the data as we proceed with our project.\n"
      ],
      "metadata": {
        "id": "jMNFueXhbNJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHtREloLxko1"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def read_hdf5_in_batches(file_path, batch_size=10):\n",
        "    with h5py.File(file_path, 'r') as h5f:\n",
        "        num_images = len(h5f.keys())\n",
        "        for start in range(0, num_images, batch_size):\n",
        "            end = min(start + batch_size, num_images)\n",
        "            data_batch = {}\n",
        "            for i in range(start, end):\n",
        "                group = h5f[f'image_{i}']\n",
        "                result_matrix = group['result_matrix'][:]\n",
        "                meta = {key: group['meta'].attrs[key] for key in group['meta'].attrs}\n",
        "\n",
        "                # Read thumbnail and bands if they exist\n",
        "                thumbnail = group['thumbnail'][:] if 'thumbnail' in group else None\n",
        "                bands = group['bands'][:] if 'bands' in group else None\n",
        "\n",
        "                data_batch[i] = {\n",
        "                    'result_matrix': result_matrix,\n",
        "                    'meta': meta,\n",
        "                    'thumbnail': thumbnail,\n",
        "                    'bands': bands\n",
        "                }\n",
        "            yield data_batch\n",
        "\n",
        "def convert_to_dataframe(data_dict):\n",
        "    # Create a list of dictionaries\n",
        "    data_list = []\n",
        "    for idx, data in data_dict.items():\n",
        "        data_list.append({\n",
        "            'result_matrix': data['result_matrix'],\n",
        "            'meta': data['meta'],\n",
        "            'thumbnail': data['thumbnail'],\n",
        "            'bands': data['bands']\n",
        "        })\n",
        "\n",
        "    # Convert list of dictionaries to DataFrame\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_image_pairs_to_drive(data_df, drive_path):\n",
        "    if not os.path.exists(drive_path):\n",
        "        os.makedirs(drive_path)\n",
        "\n",
        "    crop_colors = {\n",
        "        b'po': (255, 0, 0),    # Red\n",
        "        b'or': (0, 255, 255),  # Cyan\n",
        "        b'pe': (255, 0, 255),  # Magenta\n",
        "        b'gr': (0, 255, 0),    # Green\n",
        "        b'ot': (255, 255, 0),  # Yellow\n",
        "        b'ma': (0, 0, 255),    # Blue\n",
        "        b'sb': (128, 0, 128),  # Purple\n",
        "        b'sw': (0, 128, 128),  # Teal\n",
        "        b'wb': (255, 165, 0),  # Orange\n",
        "        b'wo': (128, 128, 0),  # Olive\n",
        "        b'ww': (75, 0, 130),   # Indigo\n",
        "        b'sl': (255, 105, 180),# Hot Pink\n",
        "        b'so': (64, 224, 208), # Turquoise\n",
        "        b'fs': (0, 255, 127),  # Spring Green\n",
        "        b'fw': (218, 112, 214),# Orchid\n",
        "        b'un': (255, 255, 255) # White\n",
        "    }\n",
        "\n",
        "    for idx, row in data_df.iterrows():\n",
        "        thumbnail = row['thumbnail']\n",
        "        result_matrix = row['result_matrix']\n",
        "\n",
        "        if thumbnail is not None and result_matrix is not None:\n",
        "            # Print initial values of result_matrix\n",
        "            print(f\"Initial result_matrix values for image {idx}:\\n{result_matrix}\")\n",
        "\n",
        "            # Print unique values in result_matrix to verify crop types\n",
        "            unique_values = np.unique(result_matrix)\n",
        "            print(f\"Unique crop types in result_matrix for image {idx}: {unique_values}\")\n",
        "\n",
        "            # Create mask from result_matrix\n",
        "            mask = np.zeros((*result_matrix.shape, 3), dtype=np.uint8)\n",
        "            for crop_type, color in crop_colors.items():\n",
        "                mask[result_matrix == crop_type] = color\n",
        "\n",
        "            # Print the mask to ensure colors are assigned correctly\n",
        "            unique_colors = np.unique(mask.reshape(-1, mask.shape[2]), axis=0)\n",
        "            print(f\"Unique colors in mask for image {idx}: {unique_colors}\")\n",
        "\n",
        "            # Convert arrays to images\n",
        "            thumbnail_img = Image.fromarray((thumbnail * 255).astype(np.uint8).transpose(1, 2, 0))\n",
        "            mask_img = Image.fromarray(mask)\n",
        "\n",
        "            # Save images to Google Drive\n",
        "            thumbnail_img.save(os.path.join(drive_path, f'thumbnail_{idx}.png'))\n",
        "            mask_img.save(os.path.join(drive_path, f'mask_{idx}.png'))\n",
        "\n",
        "            print(f\"Saved pair {idx + 1}/{len(data_df)}\")\n",
        "        else:\n",
        "            print(f\"Skipping image {idx + 1}/{len(data_df)} due to missing data\")\n",
        "\n",
        "# Path to the saved HDF5 file\n",
        "hdf5_file_path = '/content/drive/My Drive/IntermediateResults/all_data.h5'\n",
        "\n",
        "# Path to save images on Google Drive\n",
        "drive_path = '/content/drive/My Drive/MscProject/ImageMaskPairs2023'\n",
        "\n",
        "# Read the HDF5 file in batches\n",
        "batch_size = 10\n",
        "batch_number = 0\n",
        "for data_batch in read_hdf5_in_batches(hdf5_file_path, batch_size=batch_size):\n",
        "    data_df = convert_to_dataframe(data_batch)\n",
        "    save_image_pairs_to_drive(data_df, os.path.join(drive_path, f'batch_{batch_number}'))\n",
        "    batch_number += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwZk4+iCZAuxXl+w3B9CvK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}